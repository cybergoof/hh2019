---
layout: objective
title: "Frido Sleigh Contest"
#date: 2018-12-03
number: "8"
#image: "challenge.md"
question: "Get past Capta for Frido Sleigh "
answer: "8Ia8LiZEwvyZr2WO"
images: ["1.png"]
narrative: ""
tags: [sample post, readability, test]
comments: false
image-dir: /assets/img/objectives/8
links: ["https://www.youtube.com/watch?v=jmVPLwjm_zs","https://downloads.elfu.org/capteha_images.tar.gz","https://downloads.elfu.org/capteha_api.py","https://github.com/chrisjd20/img_rec_tf_ml_demo"]
comments: false
---
<figure>
	<img src="/assets/img/objectives/8/question.jpg">
	<figcaption>Question</figcaption>
</figure>

I have to beat the Capta on the Frido Sleigh website.  Luckily, Krampus provided links to the images to train on, and the
API interface built in Python.

I need to use machine learning to train a model to detect the image.  The Kringlecon talk includes a link to a GitHub repo that has 
a tensorflow based code that will train the model.


The code provided did a good job of providing the two pieces.  

Once the image were trained, the following [code](file://assets/files/cap.py) was used to grab the web information, call the predictor, filter out the matching UUID, and then post back the response.

An email with the proper code was sent.

<figure>
	<img src="/assets/img/objectives/8/answer.jpg">
	<figcaption>Answer</figcaption>
</figure>



<figure>
	<img src="/assets/img/objectives/8/answer.jpg">
	<figcaption>Narrative</figcaption>
</figure>

A new part of the narrative opens

<figure>
	<img src="/assets/img/narratives/6.png">
	<figcaption>Narrative</figcaption>
</figure>

